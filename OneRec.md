# OneRec: Unifying Retrieve and Rank with Generative Recommender and Preference Alignment

**é“¾æ¥**ï¼š[arXiv 2502.18965](https://arxiv.org/abs/2502.18965)

---

## ğŸ”‘ æ ¸å¿ƒåšæ³•

### 1. ç”Ÿæˆå¼æ¨è

å°†æ¨èå»ºæ¨¡ä¸ºåºåˆ—ç”Ÿæˆä»»åŠ¡ï¼šç»™å®šç”¨æˆ·å†å²è¡Œä¸ºåºåˆ—ï¼Œæ¨¡å‹ç›´æ¥ç”Ÿæˆä¸€ä¸ª item åºåˆ—ä½œä¸ºæ¨èç»“æœï¼Œç»Ÿä¸€å¬å›å’Œæ’åºæµç¨‹ã€‚  
item è¢«ç¦»æ•£åŒ–ä¸º tokenï¼Œtoken ç©ºé—´é€šè¿‡ **Balanced K-means clustering** ä» embedding ä¸­æ„å»ºã€‚

---

### 2. æ¨¡å‹è®­ç»ƒæµç¨‹

- **é¢„è®­ç»ƒ**ï¼šåœ¨é«˜è´¨é‡ session æ•°æ®ä¸Šï¼Œç”¨ next-item prediction ç›®æ ‡è®­ç»ƒç”Ÿæˆæ¨¡å‹ï¼ˆTransformer decoderï¼‰ã€‚
- **åå¥½å¯¹é½ï¼ˆPreference Alignmentï¼‰**ï¼š
  - å•ç‹¬è®­ç»ƒä¸€ä¸ª **reward model**ï¼Œé¢„æµ‹ä»¥ä¸‹å››ä¸ªç›®æ ‡çš„æœŸæœ›å€¼ï¼š
    - Session watch time
    - View probability
    - Follow probability
    - Like probability
  - ç”¨è¯¥æ¨¡å‹å¯¹å€™é€‰æ¨èåºåˆ—æ‰“åˆ†ï¼Œä½œä¸ºåç»­å¼ºåŒ–ä¼˜åŒ–ä¿¡å·ã€‚

---

### 3. DPO ä¼˜åŒ–æ–¹æ³•

- ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ + beam search ç”Ÿæˆå¤šä¸ªæ¨èåºåˆ—
- ç”¨ reward model æ‰“åˆ†ï¼ŒæŒ‘é€‰å¾—åˆ†é«˜/ä½çš„æ­£è´Ÿæ ·æœ¬ \( y^+, y^- \)
- ä¼˜åŒ– decoderï¼Œä½¿å…¶æ›´åå¥½é«˜è¯„åˆ†åºåˆ—ï¼š

$$
\mathcal{L}_{\text{DPO}} = - \log \frac{ \exp( r(y^+) / \beta ) }{ \exp( r(y^+) / \beta ) + \exp( r(y^-) / \beta ) }
$$

å…¶ä¸­ \( r(\cdot) \) ä¸º rewardï¼Œ\( \beta \) ä¸ºæ¸©åº¦ç³»æ•°ã€‚

---

## ğŸ“ˆ æ•ˆæœ

- OneRec åœ¨æ— éœ€ä¸¤é˜¶æ®µå¬å›æ’åºçš„æƒ…å†µä¸‹è¾¾åˆ°æ›´ä¼˜æ¨èæ€§èƒ½
- Preference Alignment æé«˜äº†æ¨èç»“æœåœ¨æ»¡æ„åº¦ã€å¤šæ ·æ€§ã€ç›¸å…³æ€§ä¸Šçš„å¹³è¡¡æ€§
